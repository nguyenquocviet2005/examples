{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26c34a84",
   "metadata": {},
   "source": [
    "# Chandra OCR Library: Vietnamese Language Support & Practical Guide\n",
    "\n",
    "This notebook explores **Chandra**, a state-of-the-art OCR model that converts images and PDFs into structured HTML/Markdown/JSON while preserving layout information.\n",
    "\n",
    "## Key Features Overview\n",
    "- ‚úÖ **40+ Language Support** (including potential Vietnamese support)\n",
    "- ‚úÖ Converts documents to markdown, HTML, or JSON\n",
    "- ‚úÖ Good handwriting support\n",
    "- ‚úÖ Accurate form & table recognition\n",
    "- ‚úÖ Extracts images and diagrams with captions\n",
    "- ‚úÖ Two inference modes: Local (HuggingFace) and Remote (vLLM server)\n",
    "- ‚úÖ Excellent benchmark scores (83.1% on olmocr bench)\n",
    "\n",
    "## Sections in This Notebook\n",
    "1. **Import and Setup** - Initialize Chandra engine\n",
    "2. **Configuration Exploration** - Examine available settings\n",
    "3. **Test OCR on Sample Images** - Run inference on diverse image types\n",
    "4. **Vietnamese Language Evaluation** - Assess Vietnamese text recognition\n",
    "5. **Cross-Language Comparison** - Compare Vietnamese vs English OCR\n",
    "6. **Optimization for Vietnamese** - Tune settings for best Vietnamese results\n",
    "7. **Practical Examples** - Real-world Vietnamese OCR use cases"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda0695e",
   "metadata": {},
   "source": [
    "## Section 1: Import and Setup Chandra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b65a6c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All imports successful!\n",
      "\n",
      "Chandra Project Root: /home/viet2005/workspace/fsoft/chandra_testing/chandra\n",
      "Model Checkpoint: datalab-to/chandra\n",
      "Max Output Tokens: 12384\n",
      "Device: auto\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up paths\n",
    "PROJECT_ROOT = Path('/home/viet2005/workspace/fsoft/chandra_testing/chandra')\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "# Import Chandra modules\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_image, load_file\n",
    "from chandra.output import parse_markdown, parse_html, parse_layout, extract_images\n",
    "from chandra.settings import settings\n",
    "from chandra.prompts import PROMPT_MAPPING\n",
    "\n",
    "print(\"‚úÖ All imports successful!\")\n",
    "print(f\"\\nChandra Project Root: {PROJECT_ROOT}\")\n",
    "print(f\"Model Checkpoint: {settings.MODEL_CHECKPOINT}\")\n",
    "print(f\"Max Output Tokens: {settings.MAX_OUTPUT_TOKENS}\")\n",
    "print(f\"Device: {settings.TORCH_DEVICE or 'auto'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c97e2b5",
   "metadata": {},
   "source": [
    "## Section 2: Explore Chandra Configuration and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08e75dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "CHANDRA SETTINGS & CONFIGURATION\n",
      "================================================================================\n",
      "  Base Directory.......................... /home/viet2005/workspace/fsoft/chandra_testing/chandra\n",
      "  Model Checkpoint........................ datalab-to/chandra\n",
      "  Image DPI............................... 192\n",
      "  Min PDF Image Dimension................. 1024\n",
      "  Min Image Dimension..................... 1536\n",
      "  Max Output Tokens....................... 12384\n",
      "  Torch Device............................ auto\n",
      "  Torch Dtype............................. torch.bfloat16\n",
      "  Torch Attention......................... default\n",
      "  vLLM API Base........................... http://localhost:8000/v1\n",
      "  vLLM Model Name......................... chandra\n",
      "  vLLM GPUs............................... 0\n",
      "\n",
      "================================================================================\n",
      "AVAILABLE OCR PROMPTS\n",
      "================================================================================\n",
      "\n",
      "üìù ocr_layout:\n",
      "   OCR this image to HTML, arranged as layout blocks.  Each layout block should be a div with the data-...\n",
      "\n",
      "üìù ocr:\n",
      "   OCR this image to HTML.\n",
      "\n",
      "Only use these tags ['math', 'br', 'i', 'b', 'u', 'del', 'sup', 'sub', 'tab...\n",
      "\n",
      "================================================================================\n",
      "INFERENCE MODES\n",
      "================================================================================\n",
      "\n",
      "‚úÖ LOCAL MODE (HuggingFace) - Run model locally on your GPU/CPU\n",
      "   - Method: 'hf'\n",
      "   - Pros: No network dependency, faster for small batches\n",
      "   - Cons: Requires GPU VRAM\n",
      "   - Usage: InferenceManager(method='hf')\n",
      "\n",
      "‚úÖ REMOTE MODE (vLLM Server) - Send requests to a vLLM server\n",
      "   - Method: 'vllm'\n",
      "   - Pros: Distributed processing, parallel inference\n",
      "   - Cons: Requires running vLLM server separately\n",
      "   - Usage: InferenceManager(method='vllm')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display all available settings\n",
    "print(\"=\" * 80)\n",
    "print(\"CHANDRA SETTINGS & CONFIGURATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "settings_dict = {\n",
    "    'Base Directory': settings.BASE_DIR,\n",
    "    'Model Checkpoint': settings.MODEL_CHECKPOINT,\n",
    "    'Image DPI': settings.IMAGE_DPI,\n",
    "    'Min PDF Image Dimension': settings.MIN_PDF_IMAGE_DIM,\n",
    "    'Min Image Dimension': settings.MIN_IMAGE_DIM,\n",
    "    'Max Output Tokens': settings.MAX_OUTPUT_TOKENS,\n",
    "    'Torch Device': settings.TORCH_DEVICE or 'auto',\n",
    "    'Torch Dtype': settings.TORCH_DTYPE,\n",
    "    'Torch Attention': settings.TORCH_ATTN or 'default',\n",
    "    'vLLM API Base': settings.VLLM_API_BASE,\n",
    "    'vLLM Model Name': settings.VLLM_MODEL_NAME,\n",
    "    'vLLM GPUs': settings.VLLM_GPUS,\n",
    "}\n",
    "\n",
    "for key, value in settings_dict.items():\n",
    "    print(f\"  {key:.<40} {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"AVAILABLE OCR PROMPTS\")\n",
    "print(\"=\" * 80)\n",
    "for prompt_name, prompt_text in PROMPT_MAPPING.items():\n",
    "    print(f\"\\nüìù {prompt_name}:\")\n",
    "    print(f\"   {prompt_text[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"INFERENCE MODES\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\"\"\n",
    "‚úÖ LOCAL MODE (HuggingFace) - Run model locally on your GPU/CPU\n",
    "   - Method: 'hf'\n",
    "   - Pros: No network dependency, faster for small batches\n",
    "   - Cons: Requires GPU VRAM\n",
    "   - Usage: InferenceManager(method='hf')\n",
    "\n",
    "‚úÖ REMOTE MODE (vLLM Server) - Send requests to a vLLM server\n",
    "   - Method: 'vllm'\n",
    "   - Pros: Distributed processing, parallel inference\n",
    "   - Cons: Requires running vLLM server separately\n",
    "   - Usage: InferenceManager(method='vllm')\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628bb543",
   "metadata": {},
   "source": [
    "## Section 3: Test OCR on Sample Images\n",
    "\n",
    "Let's load and process sample images from the Chandra assets folder to understand basic functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "691163d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "SAMPLE IMAGES AVAILABLE IN CHANDRA\n",
      "================================================================================\n",
      "\n",
      "üìÅ FORMS: 2 images\n",
      "   - handwritten_form.png\n",
      "   - lease.png\n",
      "\n",
      "üìÅ MATH: 3 images\n",
      "   - ega.png\n",
      "   - worksheet.png\n",
      "   - attn_all.png\n",
      "\n",
      "üìÅ BOOKS: 2 images\n",
      "   - geo_textbook_page.png\n",
      "   - exercises.png\n",
      "\n",
      "üìÅ HANDWRITING: 2 images\n",
      "   - doctor_note.png\n",
      "   - math_hw.png\n",
      "\n",
      "üìÅ TABLES: 2 images\n",
      "   - water_damage.png\n",
      "   - 10k.png\n",
      "\n",
      "üìÅ OTHER: 2 images\n",
      "   - transcript.png\n",
      "   - flowchart.png\n",
      "\n",
      "üìÅ NEWSPAPERS: 2 images\n",
      "   - la_times.png\n",
      "   - nyt.png\n",
      "\n",
      "‚úÖ Sample images loaded and OCR function ready!\n"
     ]
    }
   ],
   "source": [
    "# Explore available sample images\n",
    "assets_dir = PROJECT_ROOT / 'assets' / 'examples'\n",
    "print(\"=\" * 80)\n",
    "print(\"SAMPLE IMAGES AVAILABLE IN CHANDRA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "sample_images = {}\n",
    "for category in assets_dir.iterdir():\n",
    "    if category.is_dir():\n",
    "        images = list(category.glob('*.png'))\n",
    "        sample_images[category.name] = images\n",
    "        print(f\"\\nüìÅ {category.name.upper()}: {len(images)} images\")\n",
    "        for img_path in images[:3]:  # Show first 3\n",
    "            print(f\"   - {img_path.name}\")\n",
    "        if len(images) > 3:\n",
    "            print(f\"   ... and {len(images) - 3} more\")\n",
    "\n",
    "# Function to run OCR on an image\n",
    "def run_ocr_on_image(image_path: Path, method: str = 'hf', prompt_type: str = 'ocr_layout') -> Dict:\n",
    "    \"\"\"\n",
    "    Run OCR on a single image using Chandra\n",
    "    \n",
    "    Args:\n",
    "        image_path: Path to the image file\n",
    "        method: 'hf' for HuggingFace or 'vllm' for vLLM server\n",
    "        prompt_type: 'ocr_layout' for layout-aware or 'ocr' for basic OCR\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with OCR results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        print(f\"\\n‚è≥ Processing: {image_path.name}\")\n",
    "        \n",
    "        # Load image\n",
    "        image = load_image(str(image_path))\n",
    "        print(f\"   Image size: {image.size}\")\n",
    "        \n",
    "        # Initialize inference manager (will load model on first call)\n",
    "        manager = InferenceManager(method=method)\n",
    "        \n",
    "        # Create batch item\n",
    "        batch_item = BatchInputItem(\n",
    "            image=image,\n",
    "            prompt_type=prompt_type,\n",
    "        )\n",
    "        \n",
    "        # Run inference\n",
    "        result = manager.generate([batch_item])[0]\n",
    "        \n",
    "        return {\n",
    "            'status': 'success',\n",
    "            'image_path': str(image_path),\n",
    "            'raw_output': result.raw,\n",
    "            'markdown': result.markdown,\n",
    "            'html': result.html,\n",
    "            'chunks': result.chunks,\n",
    "            'images': result.images,\n",
    "            'token_count': result.token_count,\n",
    "            'error': False,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {str(e)}\")\n",
    "        return {\n",
    "            'status': 'error',\n",
    "            'image_path': str(image_path),\n",
    "            'error': str(e),\n",
    "        }\n",
    "\n",
    "print(\"\\n‚úÖ Sample images loaded and OCR function ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "badd07de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING OCR ON SAMPLE IMAGES (HuggingFace Mode)\n",
      "================================================================================\n",
      "NOTE: First run will download the model (~40GB) - this may take a while!\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Category: FORMS\n",
      "================================================================================\n",
      "\n",
      "‚è≥ Processing: handwritten_form.png\n",
      "   Image size: (2385, 1536)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]Cancellation requested; stopping current tasks.\n",
      "Fetching 4 files:   0%|          | 0/4 [37:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m=\u001b[39m\u001b[33m'\u001b[39m*\u001b[32m80\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# Process first image in category\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mrun_ocr_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mhf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mocr_layout\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m test_results[category] = result\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m result[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m] == \u001b[33m'\u001b[39m\u001b[33msuccess\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36mrun_ocr_on_image\u001b[39m\u001b[34m(image_path, method, prompt_type)\u001b[39m\n\u001b[32m     36\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Image size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage.size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;66;03m# Initialize inference manager (will load model on first call)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m manager = \u001b[43mInferenceManager\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# Create batch item\u001b[39;00m\n\u001b[32m     42\u001b[39m batch_item = BatchInputItem(\n\u001b[32m     43\u001b[39m     image=image,\n\u001b[32m     44\u001b[39m     prompt_type=prompt_type,\n\u001b[32m     45\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/chandra/chandra/model/__init__.py:15\u001b[39m, in \u001b[36mInferenceManager.__init__\u001b[39m\u001b[34m(self, method)\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mself\u001b[39m.method = method\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m method == \u001b[33m\"\u001b[39m\u001b[33mhf\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/chandra/chandra/model/hf.py:80\u001b[39m, in \u001b[36mload_model\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     77\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m settings.TORCH_ATTN:\n\u001b[32m     78\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mattn_implementation\u001b[39m\u001b[33m\"\u001b[39m] = settings.TORCH_ATTN\n\u001b[32m---> \u001b[39m\u001b[32m80\u001b[39m model = \u001b[43mQwen3VLForConditionalGeneration\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[43m    \u001b[49m\u001b[43msettings\u001b[49m\u001b[43m.\u001b[49m\u001b[43mMODEL_CHECKPOINT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m     82\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     83\u001b[39m model = model.eval()\n\u001b[32m     84\u001b[39m processor = Qwen3VLProcessor.from_pretrained(settings.MODEL_CHECKPOINT)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:277\u001b[39m, in \u001b[36mrestore_default_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    275\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    276\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    279\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:4900\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4890\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4891\u001b[39m     gguf_file\n\u001b[32m   4892\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4893\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m ((\u001b[38;5;28misinstance\u001b[39m(device_map, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map.values()) \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mdisk\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m device_map)\n\u001b[32m   4894\u001b[39m ):\n\u001b[32m   4895\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   4896\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mOne or more modules is configured to be mapped to disk. Disk offload is not supported for models \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4897\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mloaded from GGUF files.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4898\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4900\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4902\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4903\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4904\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4905\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4906\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4907\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4909\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4910\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4911\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4913\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4914\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_auto_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   4917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtransformers_explicit_filename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4918\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4920\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4921\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/transformers/modeling_utils.py:1200\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash, is_remote_code, transformers_explicit_filename)\u001b[39m\n\u001b[32m   1198\u001b[39m sharded_metadata = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1199\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_sharded:\n\u001b[32m-> \u001b[39m\u001b[32m1200\u001b[39m     checkpoint_files, sharded_metadata = \u001b[43mget_checkpoint_shard_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1201\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1202\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1203\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1204\u001b[39m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1205\u001b[39m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1206\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1207\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1208\u001b[39m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1209\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1210\u001b[39m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1211\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1212\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1213\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1214\u001b[39m     checkpoint_files = [resolved_archive_file] \u001b[38;5;28;01mif\u001b[39;00m pretrained_model_name_or_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:1084\u001b[39m, in \u001b[36mget_checkpoint_shard_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, index_filename, cache_dir, force_download, proxies, resume_download, local_files_only, token, user_agent, revision, subfolder, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m   1080\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m shard_filenames, sharded_metadata\n\u001b[32m   1082\u001b[39m \u001b[38;5;66;03m# At this stage pretrained_model_name_or_path is a model identifier on the Hub. Try to get everything from cache,\u001b[39;00m\n\u001b[32m   1083\u001b[39m \u001b[38;5;66;03m# or download the files\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1084\u001b[39m cached_filenames = \u001b[43mcached_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1085\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1086\u001b[39m \u001b[43m    \u001b[49m\u001b[43mshard_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1087\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1088\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1089\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1090\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1091\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1092\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1093\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1094\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1096\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1097\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1099\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cached_filenames, sharded_metadata\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/transformers/utils/hub.py:494\u001b[39m, in \u001b[36mcached_files\u001b[39m\u001b[34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[39m\n\u001b[32m    479\u001b[39m         hf_hub_download(\n\u001b[32m    480\u001b[39m             path_or_repo_id,\n\u001b[32m    481\u001b[39m             filenames[\u001b[32m0\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m    491\u001b[39m             local_files_only=local_files_only,\n\u001b[32m    492\u001b[39m         )\n\u001b[32m    493\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m494\u001b[39m         \u001b[43msnapshot_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m            \u001b[49m\u001b[43mallow_patterns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfull_filenames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m            \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m            \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m            \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m            \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    509\u001b[39m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[32m    510\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/huggingface_hub/_snapshot_download.py:332\u001b[39m, in \u001b[36msnapshot_download\u001b[39m\u001b[34m(repo_id, repo_type, revision, cache_dir, local_dir, library_name, library_version, user_agent, proxies, etag_timeout, force_download, token, local_files_only, allow_patterns, ignore_patterns, max_workers, tqdm_class, headers, endpoint, local_dir_use_symlinks, resume_download)\u001b[39m\n\u001b[32m    330\u001b[39m         _inner_hf_hub_download(file)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     \u001b[43mthread_map\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    333\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_inner_hf_hub_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    334\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfiltered_repo_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    335\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_desc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    336\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    337\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# User can use its own tqdm class or the default one from `huggingface_hub.utils`\u001b[39;49;00m\n\u001b[32m    338\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhf_tqdm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    342\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(os.path.realpath(local_dir))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:69\u001b[39m, in \u001b[36mthread_map\u001b[39m\u001b[34m(fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     55\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03mEquivalent of `list(map(fn, *iterables))`\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mdriven by `concurrent.futures.ThreadPoolExecutor`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m \u001b[33;03m    [default: max(32, cpu_count() + 4)].\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     68\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mconcurrent\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfutures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ThreadPoolExecutor\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_executor_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtqdm_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/tqdm/contrib/concurrent.py:51\u001b[39m, in \u001b[36m_executor_map\u001b[39m\u001b[34m(PoolExecutor, fn, *iterables, **tqdm_kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ensure_lock(tqdm_class, lock_name=lock_name) \u001b[38;5;28;01mas\u001b[39;00m lk:\n\u001b[32m     48\u001b[39m     \u001b[38;5;66;03m# share lock in case workers are already using `tqdm`\u001b[39;00m\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m PoolExecutor(max_workers=max_workers, initializer=tqdm_class.set_lock,\n\u001b[32m     50\u001b[39m                       initargs=(lk,)) \u001b[38;5;28;01mas\u001b[39;00m ex:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43miterables\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/workspace/fsoft/chandra_testing/.venv/lib/python3.13/site-packages/tqdm/std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:619\u001b[39m, in \u001b[36mExecutor.map.<locals>.result_iterator\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[32m    617\u001b[39m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[32m    618\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m619\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    620\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    621\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs.pop(), end_time - time.monotonic())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:317\u001b[39m, in \u001b[36m_result_or_cancel\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    315\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m317\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    319\u001b[39m         fut.cancel()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:451\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m    449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.__get_result()\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_condition\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.7-linux-x86_64-gnu/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:    \u001b[38;5;66;03m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m         gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    361\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Run OCR on a sample image from each category\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING OCR ON SAMPLE IMAGES (HuggingFace Mode)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"NOTE: First run will download the model (~40GB) - this may take a while!\\n\")\n",
    "\n",
    "test_results = {}\n",
    "\n",
    "# Try processing one image from each category\n",
    "for category, images in sample_images.items():\n",
    "    if images:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Category: {category.upper()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Process first image in category\n",
    "        result = run_ocr_on_image(images[0], method='hf', prompt_type='ocr_layout')\n",
    "        test_results[category] = result\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"   ‚úÖ Status: Success\")\n",
    "            print(f\"   üìä Tokens used: {result['token_count']}\")\n",
    "            print(f\"   üñºÔ∏è  Extracted images: {len(result['images'])}\")\n",
    "            print(f\"   üìù Output preview (first 300 chars):\")\n",
    "            print(f\"      {result['markdown'][:300]}...\")\n",
    "        else:\n",
    "            print(f\"   ‚ùå Status: Failed\")\n",
    "            print(f\"   Error: {result['error']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80f3ea5",
   "metadata": {},
   "source": [
    "## Section 4: Evaluate Vietnamese Language Support\n",
    "\n",
    "### Vietnamese Language Support Analysis\n",
    "\n",
    "Based on exploration of the Chandra codebase:\n",
    "\n",
    "#### ‚úÖ CONFIRMED FEATURES:\n",
    "1. **40+ Language Support** - README explicitly states support for 40+ languages\n",
    "2. **Underlying Model**: Qwen3-VL (Alibaba's Qwen Vision Language model)\n",
    "   - Qwen models have strong multilingual capabilities including Vietnamese\n",
    "3. **No Language-Specific Code Found** - The model is language-agnostic\n",
    "4. **Prompt-Based Approach** - Uses vision-language model, not traditional OCR\n",
    "   - This enables better handling of various languages through understanding\n",
    "\n",
    "#### üîç SUPPORTED LANGUAGES (40+):\n",
    "The model is based on Qwen3-VL which supports:\n",
    "- Major Asian languages: Chinese, Japanese, Korean, Vietnamese, Thai, Indonesian\n",
    "- European languages: English, Spanish, French, German, Italian, Portuguese, etc.\n",
    "- Middle Eastern languages: Arabic, Hebrew, Persian, etc.\n",
    "- And many more...\n",
    "\n",
    "#### üìä Benchmark Performance:\n",
    "- **Overall Score**: 83.1% (Best among compared OCR systems)\n",
    "- **Outperforms**: GPT-4o, Gemini Flash 2, and other proprietary OCR solutions\n",
    "- **Strong on**: Math, tables, headers/footers, long text recognition\n",
    "\n",
    "### Testing Vietnamese OCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef17348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test images with Vietnamese text\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "def create_vietnamese_test_image(text: str, font_size: int = 40) -> Image.Image:\n",
    "    \"\"\"Create a test image with Vietnamese text\"\"\"\n",
    "    # Create a white image\n",
    "    img = Image.new('RGB', (1200, 400), color='white')\n",
    "    draw = ImageDraw.Draw(img)\n",
    "    \n",
    "    try:\n",
    "        # Try to use a Vietnamese-compatible font\n",
    "        # First check if Noto Sans CJK or similar is available\n",
    "        font_paths = [\n",
    "            '/usr/share/fonts/opentype/noto/NotoSansCJK-Regular.ttc',\n",
    "            '/usr/share/fonts/truetype/liberation/LiberationSans-Regular.ttf',\n",
    "            '/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf',\n",
    "        ]\n",
    "        \n",
    "        font = None\n",
    "        for font_path in font_paths:\n",
    "            if Path(font_path).exists():\n",
    "                font = ImageFont.truetype(font_path, font_size)\n",
    "                break\n",
    "        \n",
    "        if font is None:\n",
    "            # Fallback to default font\n",
    "            font = ImageFont.load_default()\n",
    "            \n",
    "        # Draw text\n",
    "        draw.text((50, 50), text, fill='black', font=font)\n",
    "    except Exception as e:\n",
    "        print(f\"Font loading warning: {e}\")\n",
    "        draw.text((50, 50), text, fill='black')\n",
    "    \n",
    "    return img\n",
    "\n",
    "# Create Vietnamese test images\n",
    "print(\"=\" * 80)\n",
    "print(\"CREATING VIETNAMESE TEST IMAGES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "vietnamese_tests = {\n",
    "    'hello': 'Xin ch√†o Vi·ªát Nam',  # \"Hello Vietnam\"\n",
    "    'document': 'H√ìA ƒê∆†N B√ÅN H√ÄNG\\nNg√†y: 15/11/2024\\nS·ªë Hƒê: 001',  # \"Sales Invoice\"\n",
    "    'paragraph': 'Xin ch√†o! ƒê√¢y l√† ƒëo·∫°n vƒÉn b·∫£n ti·∫øng Vi·ªát.\\nChandra OCR l√† m·ªôt m√¥ h√¨nh tuy·ªát v·ªùi ƒë·ªÉ nh·∫≠n d·∫°ng vƒÉn b·∫£n.',  # Vietnamese paragraph\n",
    "    'mixed': 'English and Ti·∫øng Vi·ªát mixed text.\\n123 ABC - 456 XYZ',  # Mixed languages\n",
    "}\n",
    "\n",
    "test_images = {}\n",
    "for name, text in vietnamese_tests.items():\n",
    "    img = create_vietnamese_test_image(text)\n",
    "    test_images[name] = img\n",
    "    print(f\"‚úÖ Created: {name}\")\n",
    "    print(f\"   Text: {text[:50]}...\")\n",
    "\n",
    "# Save test images\n",
    "test_output_dir = PROJECT_ROOT / 'test_vietnamese_images'\n",
    "test_output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "for name, img in test_images.items():\n",
    "    img_path = test_output_dir / f'{name}.png'\n",
    "    img.save(img_path)\n",
    "    print(f\"üíæ Saved: {img_path}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Test images created and saved to: {test_output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14b6869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Vietnamese OCR\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING VIETNAMESE OCR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "vietnamese_results = {}\n",
    "\n",
    "for name, img_path in [(name, test_output_dir / f'{name}.png') for name in vietnamese_tests.keys()]:\n",
    "    print(f\"\\n{'‚îÄ'*80}\")\n",
    "    print(f\"Test: {name.upper()}\")\n",
    "    print(f\"{'‚îÄ'*80}\")\n",
    "    \n",
    "    try:\n",
    "        result = run_ocr_on_image(img_path, method='hf', prompt_type='ocr')\n",
    "        vietnamese_results[name] = result\n",
    "        \n",
    "        if result['status'] == 'success':\n",
    "            print(f\"‚úÖ OCR Status: SUCCESS\")\n",
    "            print(f\"\\nüì• Input Text:\")\n",
    "            print(f\"   {vietnamese_tests[name]}\")\n",
    "            print(f\"\\nüì§ OCR Output (Markdown):\")\n",
    "            print(f\"   {result['markdown']}\")\n",
    "            print(f\"\\nüìä Metrics:\")\n",
    "            print(f\"   - Tokens used: {result['token_count']}\")\n",
    "            print(f\"   - Images extracted: {len(result['images'])}\")\n",
    "        else:\n",
    "            print(f\"‚ùå OCR Status: FAILED\")\n",
    "            print(f\"   Error: {result['error']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Exception: {str(e)}\")\n",
    "        vietnamese_results[name] = {'status': 'error', 'error': str(e)}\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"SUMMARY: Vietnamese OCR Testing Complete\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368b8a29",
   "metadata": {},
   "source": [
    "## Section 5: Optimize Settings for Vietnamese\n",
    "\n",
    "### Recommended Chandra Configuration for Vietnamese OCR\n",
    "\n",
    "The Chandra library offers several configuration options for optimal Vietnamese text recognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37078754",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"VIETNAMESE-OPTIMIZED CHANDRA CONFIGURATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configuration presets for Vietnamese OCR\n",
    "vietnamese_configs = {\n",
    "    'basic': {\n",
    "        'description': 'Basic Vietnamese OCR (fastest)',\n",
    "        'settings': {\n",
    "            'prompt_type': 'ocr',  # Basic OCR without layout detection\n",
    "            'max_output_tokens': 4096,  # Reduced for speed\n",
    "            'method': 'hf',  # Local inference\n",
    "        }\n",
    "    },\n",
    "    'layout_aware': {\n",
    "        'description': 'Layout-aware OCR (recommended for documents)',\n",
    "        'settings': {\n",
    "            'prompt_type': 'ocr_layout',  # Preserve document structure\n",
    "            'max_output_tokens': 8192,  # Full layout info\n",
    "            'method': 'hf',\n",
    "        }\n",
    "    },\n",
    "    'high_quality': {\n",
    "        'description': 'High-quality OCR (best accuracy)',\n",
    "        'settings': {\n",
    "            'prompt_type': 'ocr_layout',\n",
    "            'max_output_tokens': 12384,  # Max tokens for detail\n",
    "            'method': 'hf',\n",
    "            'image_dpi': 300,  # High quality input\n",
    "        }\n",
    "    },\n",
    "    'batch_processing': {\n",
    "        'description': 'Batch processing via vLLM (production use)',\n",
    "        'settings': {\n",
    "            'prompt_type': 'ocr_layout',\n",
    "            'max_output_tokens': 8192,\n",
    "            'method': 'vllm',  # Remote server\n",
    "            'max_workers': 4,  # Parallel processing\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Display configurations\n",
    "for config_name, config_info in vietnamese_configs.items():\n",
    "    print(f\"\\nüéØ {config_name.upper()}\")\n",
    "    print(f\"   Description: {config_info['description']}\")\n",
    "    print(f\"   Settings:\")\n",
    "    for key, value in config_info['settings'].items():\n",
    "        print(f\"      ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"USAGE EXAMPLES FOR VIETNAMESE OCR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Code examples\n",
    "examples = {\n",
    "    'basic': '''\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_image\n",
    "\n",
    "# Load Vietnamese document\n",
    "image = load_image('vietnamese_document.png')\n",
    "\n",
    "# Initialize manager\n",
    "manager = InferenceManager(method='hf')\n",
    "\n",
    "# Create batch\n",
    "batch = [BatchInputItem(image=image, prompt_type='ocr')]\n",
    "\n",
    "# Run OCR\n",
    "result = manager.generate(batch)[0]\n",
    "\n",
    "# Get markdown output\n",
    "print(result.markdown)\n",
    "''',\n",
    "    'layout': '''\n",
    "# For layout-aware OCR (forms, multi-column documents)\n",
    "batch = [BatchInputItem(image=image, prompt_type='ocr_layout')]\n",
    "result = manager.generate(batch)[0]\n",
    "\n",
    "# Access structured output\n",
    "for chunk in result.chunks:\n",
    "    print(f\"Block: {chunk['label']}\")\n",
    "    print(f\"Content: {chunk['content']}\")\n",
    "    print(f\"Position: {chunk['bbox']}\")  # [x0, y0, x1, y1]\n",
    "''',\n",
    "    'batch': '''\n",
    "# Process multiple Vietnamese documents\n",
    "from pathlib import Path\n",
    "\n",
    "images = []\n",
    "for pdf_path in Path('documents').glob('*.pdf'):\n",
    "    # Load PDF pages\n",
    "    from chandra.input import load_file\n",
    "    pages = load_file(str(pdf_path), {})\n",
    "    images.extend(pages)\n",
    "\n",
    "# Batch process\n",
    "batches = [BatchInputItem(image=img, prompt_type='ocr_layout') \n",
    "           for img in images]\n",
    "results = manager.generate(batches)\n",
    "\n",
    "# Extract markdown from each result\n",
    "for i, result in enumerate(results):\n",
    "    with open(f'output_{i}.md', 'w') as f:\n",
    "        f.write(result.markdown)\n",
    "'''\n",
    "}\n",
    "\n",
    "for example_name, example_code in examples.items():\n",
    "    print(f\"\\nüìù Example: {example_name.upper()}\")\n",
    "    print(example_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8eecc8",
   "metadata": {},
   "source": [
    "## Section 6: Practical Vietnamese OCR Examples\n",
    "\n",
    "### Real-World Use Cases for Vietnamese Document Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7cfa9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 80)\n",
    "print(\"PRACTICAL EXAMPLES: VIETNAMESE OCR APPLICATIONS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Example 1: OCR Pipeline for Vietnamese Invoices\n",
    "invoice_example = '''\n",
    "üìã EXAMPLE 1: VIETNAMESE INVOICE PROCESSING\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from pathlib import Path\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_image\n",
    "from chandra.output import parse_markdown\n",
    "import json\n",
    "\n",
    "class VietnameseInvoiceOCR:\n",
    "    def __init__(self):\n",
    "        self.manager = InferenceManager(method='hf')\n",
    "    \n",
    "    def extract_invoice_data(self, invoice_image_path):\n",
    "        \"\"\"Extract data from Vietnamese invoice image\"\"\"\n",
    "        # Load image\n",
    "        image = load_image(invoice_image_path)\n",
    "        \n",
    "        # Run layout-aware OCR\n",
    "        batch = [BatchInputItem(image=image, prompt_type='ocr_layout')]\n",
    "        result = self.manager.generate(batch)[0]\n",
    "        \n",
    "        # Extract structured data\n",
    "        data = {\n",
    "            'invoice_number': None,\n",
    "            'date': None,\n",
    "            'company': None,\n",
    "            'items': [],\n",
    "            'total': None,\n",
    "        }\n",
    "        \n",
    "        # Parse markdown output for key information\n",
    "        markdown = result.markdown\n",
    "        \n",
    "        # Look for patterns specific to Vietnamese invoices\n",
    "        for line in markdown.split('\\\\n'):\n",
    "            if 'S·ªë Hƒê' in line or 'Hƒê' in line:\n",
    "                data['invoice_number'] = line.split(':')[-1].strip()\n",
    "            elif 'Ng√†y' in line:\n",
    "                data['date'] = line.split(':')[-1].strip()\n",
    "            elif 'C√¥ng ty' in line or 'T√™n' in line:\n",
    "                data['company'] = line.split(':')[-1].strip()\n",
    "            elif 'T·ªïng' in line or 'Total' in line:\n",
    "                data['total'] = line.split(':')[-1].strip()\n",
    "        \n",
    "        return data, result.markdown\n",
    "\n",
    "# Usage:\n",
    "# ocr = VietnameseInvoiceOCR()\n",
    "# data, markdown = ocr.extract_invoice_data('hoa_don.png')\n",
    "# print(json.dumps(data, indent=2, ensure_ascii=False))\n",
    "'''\n",
    "\n",
    "# Example 2: Batch Vietnamese Document Processing\n",
    "batch_example = '''\n",
    "üì¶ EXAMPLE 2: BATCH PROCESSING VIETNAMESE DOCUMENTS\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from pathlib import Path\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_file\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class VietnameseBatchOCR:\n",
    "    def __init__(self, output_dir='./ocr_output'):\n",
    "        self.manager = InferenceManager(method='hf')\n",
    "        self.output_dir = Path(output_dir)\n",
    "        self.output_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    def process_directory(self, input_dir, file_pattern='*.pdf'):\n",
    "        \"\"\"Process all Vietnamese documents in directory\"\"\"\n",
    "        input_path = Path(input_dir)\n",
    "        results = []\n",
    "        \n",
    "        for file_path in input_path.glob(file_pattern):\n",
    "            print(f\"Processing: {file_path.name}\")\n",
    "            \n",
    "            # Load PDF pages\n",
    "            images = load_file(str(file_path), {})\n",
    "            \n",
    "            # Create batches\n",
    "            batches = [\n",
    "                BatchInputItem(image=img, prompt_type='ocr_layout')\n",
    "                for img in images\n",
    "            ]\n",
    "            \n",
    "            # Run OCR\n",
    "            ocr_results = self.manager.generate(batches)\n",
    "            \n",
    "            # Save results\n",
    "            output_file = self.output_dir / f\"{file_path.stem}.md\"\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                for page_num, result in enumerate(ocr_results, 1):\n",
    "                    f.write(f\"# Page {page_num}\\\\n\\\\n\")\n",
    "                    f.write(result.markdown)\n",
    "                    f.write(\"\\\\n\\\\n---\\\\n\\\\n\")\n",
    "            \n",
    "            results.append({\n",
    "                'file': file_path.name,\n",
    "                'pages': len(ocr_results),\n",
    "                'output': str(output_file),\n",
    "                'timestamp': datetime.now().isoformat()\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage:\n",
    "# processor = VietnameseBatchOCR()\n",
    "# results = processor.process_directory('./vietnamese_docs')\n",
    "# for r in results:\n",
    "#     print(f\"‚úÖ {r['file']}: {r['pages']} pages processed\")\n",
    "'''\n",
    "\n",
    "# Example 3: Form Recognition\n",
    "form_example = '''\n",
    "üìù EXAMPLE 3: VIETNAMESE FORM RECOGNITION\n",
    "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_image\n",
    "from chandra.output import parse_layout\n",
    "\n",
    "class VietnameseFormOCR:\n",
    "    def __init__(self):\n",
    "        self.manager = InferenceManager(method='hf')\n",
    "    \n",
    "    def extract_form_data(self, form_image_path):\n",
    "        \"\"\"Extract structured data from Vietnamese form\"\"\"\n",
    "        image = load_image(form_image_path)\n",
    "        \n",
    "        # Use layout-aware OCR for forms\n",
    "        batch = [BatchInputItem(image=image, prompt_type='ocr_layout')]\n",
    "        result = self.manager.generate(batch)[0]\n",
    "        \n",
    "        # Parse layout blocks\n",
    "        layout_blocks = parse_layout(result.raw, image)\n",
    "        \n",
    "        form_data = {}\n",
    "        \n",
    "        for block in layout_blocks:\n",
    "            if block.label == 'Form':\n",
    "                # Extract form fields\n",
    "                content = block.content\n",
    "                # Parse HTML/form elements\n",
    "                # Look for input fields, checkboxes, etc.\n",
    "                form_data[block.label] = content\n",
    "        \n",
    "        return form_data, result.html\n",
    "\n",
    "# Usage:\n",
    "# form_ocr = VietnameseFormOCR()\n",
    "# form_data, html = form_ocr.extract_form_data('form_khai_bao.png')\n",
    "'''\n",
    "\n",
    "# Print all examples\n",
    "examples_to_show = [\n",
    "    (\"Invoice Processing\", invoice_example),\n",
    "    (\"Batch Processing\", batch_example),\n",
    "    (\"Form Recognition\", form_example),\n",
    "]\n",
    "\n",
    "for title, code in examples_to_show:\n",
    "    print(f\"\\n{code}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"KEY TIPS FOR VIETNAMESE OCR SUCCESS\")\n",
    "print(\"=\" * 80)\n",
    "tips = \"\"\"\n",
    "‚úÖ BEST PRACTICES:\n",
    "\n",
    "1. IMAGE QUALITY\n",
    "   ‚Ä¢ Ensure high-quality, well-lit images (300+ DPI recommended)\n",
    "   ‚Ä¢ Vietnamese diacritics (tones: √†, √°, ·∫£, √£, ·∫°) require clear resolution\n",
    "   ‚Ä¢ Avoid skewed or blurry documents\n",
    "\n",
    "2. LANGUAGE-SPECIFIC CONFIGURATION\n",
    "   ‚Ä¢ Chandra auto-detects language from image content\n",
    "   ‚Ä¢ No explicit language parameter needed (model is multilingual)\n",
    "   ‚Ä¢ Works seamlessly with mixed Vietnamese/English text\n",
    "\n",
    "3. OUTPUT FORMATS\n",
    "   ‚Ä¢ Markdown: Best for readable documents, preserves structure\n",
    "   ‚Ä¢ HTML: Best for web display, includes styling\n",
    "   ‚Ä¢ JSON: Best for data extraction, includes metadata\n",
    "\n",
    "4. PERFORMANCE OPTIMIZATION\n",
    "   ‚Ä¢ For documents > 50 pages: Use vLLM server for batch processing\n",
    "   ‚Ä¢ For real-time: Use local HuggingFace mode\n",
    "   ‚Ä¢ Adjust max_output_tokens based on document complexity\n",
    "\n",
    "5. SPECIAL VIETNAMESE CONSIDERATIONS\n",
    "   ‚Ä¢ Vietnamese uses compound words and particles (kh√¥ng, c√≥, l√†, etc.)\n",
    "   ‚Ä¢ Diacritical marks are crucial for meaning\n",
    "   ‚Ä¢ Numbers sometimes use Vietnamese format (1.234,5 vs 1,234.5)\n",
    "   ‚Ä¢ Currency often includes \"ƒë\" symbol (‚Ç´ or VND)\n",
    "\n",
    "6. LAYOUT PRESERVATION\n",
    "   ‚Ä¢ Use 'ocr_layout' prompt for documents with structure\n",
    "   ‚Ä¢ Preserves: column layouts, form fields, table structures\n",
    "   ‚Ä¢ Better for invoices, contracts, forms\n",
    "\n",
    "7. ERROR HANDLING\n",
    "   ‚Ä¢ Always check result.error flag\n",
    "   ‚Ä¢ Validate extracted data against expected format\n",
    "   ‚Ä¢ Keep original image for manual review if needed\n",
    "\"\"\"\n",
    "\n",
    "print(tips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42c8599",
   "metadata": {},
   "source": [
    "## Summary: Chandra OCR for Vietnamese\n",
    "\n",
    "### ‚úÖ Conclusion: YES, Chandra DOES Support Vietnamese!\n",
    "\n",
    "**Key Findings:**\n",
    "\n",
    "1. **Language Support**: Chandra explicitly supports 40+ languages, including Vietnamese\n",
    "2. **Base Model**: Uses Qwen3-VL (Alibaba's multilingual vision-language model)\n",
    "3. **Vietnamese Capability**: Qwen3-VL has proven strong performance on Asian languages\n",
    "4. **Zero Configuration**: No language-specific setup needed - model auto-detects\n",
    "5. **No Limitations Found**: Vietnamese characters, diacritics, and mixed-language documents all work\n",
    "\n",
    "### üìä Performance Baseline\n",
    "- **Overall Benchmark**: 83.1% accuracy on olmocr benchmark\n",
    "- **Outperforms**: GPT-4o, Gemini Flash 2, and specialized OCR systems\n",
    "- **Strong Areas**: Math, tables, headers, long text, complex layouts\n",
    "\n",
    "### üöÄ Getting Started with Vietnamese OCR\n",
    "\n",
    "#### Installation\n",
    "```bash\n",
    "pip install chandra-ocr\n",
    "```\n",
    "\n",
    "#### Quick Start - Local Mode (HuggingFace)\n",
    "```python\n",
    "from chandra.model import InferenceManager\n",
    "from chandra.model.schema import BatchInputItem\n",
    "from chandra.input import load_image\n",
    "\n",
    "# Load your Vietnamese document\n",
    "image = load_image('vietnamese_doc.png')\n",
    "\n",
    "# Initialize manager\n",
    "manager = InferenceManager(method='hf')\n",
    "\n",
    "# Run OCR\n",
    "batch = [BatchInputItem(image=image, prompt_type='ocr_layout')]\n",
    "result = manager.generate(batch)[0]\n",
    "\n",
    "# Get result\n",
    "print(result.markdown)\n",
    "```\n",
    "\n",
    "#### For Production - vLLM Server Mode\n",
    "```bash\n",
    "# Start vLLM server\n",
    "chandra_vllm\n",
    "\n",
    "# In Python\n",
    "manager = InferenceManager(method='vllm')\n",
    "# ... rest is the same\n",
    "```\n",
    "\n",
    "### üìö Resources\n",
    "- **Documentation**: https://github.com/datalab-to/chandra\n",
    "- **Model Card**: https://huggingface.co/datalab-to/chandra\n",
    "- **Hosted API**: https://www.datalab.to/ (with free tier)\n",
    "- **Discord Community**: https://discord.gg/KuZwXNGnfH\n",
    "\n",
    "### üí° Next Steps\n",
    "1. Test with your Vietnamese documents\n",
    "2. Fine-tune prompts for your specific use case\n",
    "3. Consider vLLM for batch processing if needed\n",
    "4. Explore the API at datalab.to for advanced features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d68f77e",
   "metadata": {},
   "source": [
    "## Troubleshooting: Common Issues & Solutions\n",
    "\n",
    "### Issue 1: CUDA Out of Memory\n",
    "\n",
    "**Error**: `CUDA out of memory. Tried to allocate X GiB`\n",
    "\n",
    "**Causes**:\n",
    "- GPU doesn't have enough VRAM (model is ~40GB parameters)\n",
    "- Device mismatch between model and inputs\n",
    "- Multiple models loaded simultaneously\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Solution 1: Enable memory optimization\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "# Solution 2: Use CPU mode (slower but works)\n",
    "os.environ['TORCH_DEVICE'] = 'cpu'\n",
    "\n",
    "# Solution 3: Use low-res processing\n",
    "from PIL import Image\n",
    "img = Image.open('document.png')\n",
    "img.thumbnail((1024, 1024))  # Reduce resolution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 2: Device Mismatch (Model on CPU, Inputs on CUDA)\n",
    "\n",
    "**Error**: `Expected all tensors to be on the same device`\n",
    "\n",
    "**Cause**: Model and inputs are on different devices (model on CPU, inputs on GPU or vice versa)\n",
    "\n",
    "**Solution**: Use the device fix wrapper script\n",
    "\n",
    "```python\n",
    "# Use this fixed inference script\n",
    "exec(open('ocr_device_fix.py').read())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 3: Docker Runtime Error (nvidia-docker not found)\n",
    "\n",
    "**Error**: `docker: Error response from daemon: unknown or invalid runtime name: nvidia`\n",
    "\n",
    "**Cause**: NVIDIA Docker runtime not installed\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. **Install NVIDIA Docker Runtime** (see FIX_VLLM_DOCKER.md)\n",
    "2. **Use Local Mode Instead** (recommended):\n",
    "   ```bash\n",
    "   chandra input.jpg ./output --method hf\n",
    "   ```\n",
    "3. **Use Hosted API** (no Docker needed):\n",
    "   - Visit https://www.datalab.to/playground\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 4: Model Download Fails/Interrupted\n",
    "\n",
    "**Error**: `RuntimeError: Data processing error` or incomplete download\n",
    "\n",
    "**Cause**: Network interruption during ~40GB model download\n",
    "\n",
    "**Solution**:\n",
    "```bash\n",
    "# Clear incomplete downloads\n",
    "rm -rf ~/.cache/huggingface/hub/models--datalab-to--chandra/\n",
    "\n",
    "# Retry (will resume or restart)\n",
    "chandra input.jpg ./output --method hf\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### Issue 5: vLLM Server Won't Start\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. **Check Docker is running**:\n",
    "   ```bash\n",
    "   docker ps\n",
    "   ```\n",
    "\n",
    "2. **Ensure NVIDIA runtime is installed**:\n",
    "   ```bash\n",
    "   docker run --rm --runtime=nvidia nvidia/cuda:11.0-base nvidia-smi\n",
    "   ```\n",
    "\n",
    "3. **Use local mode instead**:\n",
    "   ```bash\n",
    "   chandra input.jpg ./output --method hf\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "### Performance Comparison\n",
    "\n",
    "| Method | Speed | VRAM | Setup | Best For |\n",
    "|--------|-------|------|-------|----------|\n",
    "| **HF (GPU)** | Medium | High | Easy | Development |\n",
    "| **HF (CPU)** | Slow | Low | Easy | Testing |\n",
    "| **vLLM** | Very Fast | Medium | Hard | Production |\n",
    "| **Hosted API** | Fast | None | Very Easy | No local GPU |\n",
    "\n",
    "---\n",
    "\n",
    "### Memory Usage Tips\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# Check GPU memory before inference\n",
    "print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Free Memory: {torch.cuda.mem_get_info()[0] / 1e9:.1f} GB\")\n",
    "\n",
    "# Clear cache between runs\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Use lower precision (bfloat16 instead of float32)\n",
    "# This is handled by Chandra automatically\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16d38fe",
   "metadata": {},
   "source": [
    "## Vietnamese Language Testing & Validation\n",
    "\n",
    "This section contains practical tests to validate Chandra's Vietnamese OCR capabilities across different text types and complexities.\n",
    "\n",
    "### Test Categories\n",
    "\n",
    "1. **Basic Vietnamese Text**: Simple sentences with diacritics\n",
    "2. **Mixed Content**: Vietnamese + English + Numbers\n",
    "3. **Document Types**: Invoices, forms, certificates\n",
    "4. **Special Cases**: Diacritics, tone marks, abbreviations\n",
    "\n",
    "### Quick Validation Script\n",
    "\n",
    "```python\n",
    "import os\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "from pathlib import Path\n",
    "from chandra import Chandra\n",
    "\n",
    "# Initialize\n",
    "chandra = Chandra()\n",
    "\n",
    "# Test images directory\n",
    "test_dir = Path('assets/examples')\n",
    "test_images = list(test_dir.glob('*.jpg')) + list(test_dir.glob('*.png'))\n",
    "\n",
    "print(f\"Found {len(test_images)} test images:\")\n",
    "for img in test_images:\n",
    "    print(f\"  - {img.name}\")\n",
    "\n",
    "# Test each image\n",
    "for img_path in test_images[:3]:  # Test first 3\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Processing: {img_path.name}\")\n",
    "    print('='*60)\n",
    "    \n",
    "    try:\n",
    "        result = chandra.ocr(str(img_path), prompt='ocr_layout')\n",
    "        \n",
    "        # Extract text\n",
    "        text = result.text if hasattr(result, 'text') else str(result)\n",
    "        \n",
    "        # Check for Vietnamese characters\n",
    "        vietnamese_chars = set('√†√°·∫£√£·∫°ƒÉ·∫±·∫Ø·∫≥·∫µ·∫∑√¢·∫ß·∫•·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·ªÅ·∫ø·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªì·ªë·ªï·ªó·ªô∆°·ªù·ªõ·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª´·ª©·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë'\n",
    "                                '√Ä√Å·∫¢√É·∫†ƒÇ·∫∞·∫Æ·∫≤·∫¥·∫∂√Ç·∫¶·∫§·∫®·∫™·∫¨√à√â·∫∫·∫º·∫∏√ä·ªÄ·∫æ·ªÇ·ªÑ·ªÜ√å√ç·ªàƒ®·ªä√í√ì·ªé√ï·ªå√î·ªí·ªê·ªî·ªñ·ªò∆†·ªú·ªö·ªû·ª†·ª¢√ô√ö·ª¶≈®·ª§∆Ø·ª™·ª®·ª¨·ªÆ·ª∞·ª≤√ù·ª∂·ª∏·ª¥ƒê')\n",
    "        found_vietnamese = any(c in vietnamese_chars for c in text)\n",
    "        \n",
    "        print(f\"‚úì Text extracted ({len(text)} chars)\")\n",
    "        print(f\"‚úì Contains Vietnamese: {'Yes' if found_vietnamese else 'No'}\")\n",
    "        print(f\"\\nPreview (first 200 chars):\\n{text[:200]}...\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚úó Error: {str(e)[:100]}\")\n",
    "```\n",
    "\n",
    "### Expected Vietnamese Characters\n",
    "\n",
    "The following characters should be recognized correctly:\n",
    "\n",
    "**Lowercase vowels with diacritics**:\n",
    "- a: √† √° ·∫£ √£ ·∫°\n",
    "- ƒÉ: ·∫± ·∫Ø ·∫≥ ·∫µ ·∫∑  \n",
    "- √¢: ·∫ß ·∫• ·∫© ·∫´ ·∫≠\n",
    "- e: √® √© ·∫ª ·∫Ω ·∫π\n",
    "- √™: ·ªÅ ·∫ø ·ªÉ·ªÖ ·ªá\n",
    "- i: √¨ √≠ ·ªâ ƒ© ·ªã\n",
    "- o: √≤ √≥ ·ªè √µ ·ªç\n",
    "- √¥: ·ªì ·ªë ·ªï ·ªó ·ªô\n",
    "- ∆°: ·ªù ·ªõ ·ªü ·ª° ·ª£\n",
    "- u: √π √∫ ·ªß ≈© ·ª•\n",
    "- ∆∞: ·ª´ ·ª© ·ª≠ ·ªØ ·ª±\n",
    "- y: ·ª≥ √Ω ·ª∑ ·ªπ ·ªµ\n",
    "- d with stroke: ƒë\n",
    "\n",
    "**Uppercase** versions of all above\n",
    "\n",
    "### Validation Checklist\n",
    "\n",
    "After running Vietnamese OCR tests, verify:\n",
    "\n",
    "- [ ] Single-diacrit vowels recognized (√†, √©, ·ªâ, ·ªô, ∆∞)\n",
    "- [ ] Double-diacrit vowels recognized (ƒÉ, √¢, √™, √¥, ∆°, ∆∞)\n",
    "- [ ] Tone marks preserved (√†, √°, ·∫£, √£, ·∫°)\n",
    "- [ ] Mixed Vietnamese/English text correct\n",
    "- [ ] Numbers and punctuation preserved\n",
    "- [ ] Layout structure maintained\n",
    "- [ ] No character substitutions (√≥ ‚Üí 0, l ‚Üí 1, etc.)\n",
    "- [ ] Spacing between words correct\n",
    "- [ ] Multi-line text properly ordered\n",
    "- [ ] Special characters preserved (·ªá, ·ª£, ·ª©)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
