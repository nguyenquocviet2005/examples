# APO Prompt Selection Visualization

## ğŸ¯ Complete Decision Flow

```
START: APO Training Begins
â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Initialize with Baseline Prompt                 â”‚
â”‚    "Answer this question: {question}"                  â”‚
â”‚         Reward Score: 0.783 (avg)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Round 1: Evaluate Baseline on Validation Set          â”‚
â”‚                                                         â”‚
â”‚  Test on 3 samples:                                    â”‚
â”‚  â€¢ "What is DNA?" â†’ Answer â†’ Reward: 0.82            â”‚
â”‚  â€¢ "What is photosynthesis?" â†’ Answer â†’ Reward: 0.78 â”‚
â”‚  â€¢ "What is machine learning?" â†’ Answer â†’ Reward: 0.75â”‚
â”‚                                                         â”‚
â”‚  Average: 0.78, Max: 0.82 â† Keep in BEAM             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Round 1: GPT-4 Critique & Generate                    â”‚
â”‚                                                         â”‚
â”‚  GPT-4 Input:                                          â”‚
â”‚  "Prompt: 'Answer this question: {question}'           â”‚
â”‚   Avg reward: 0.78                                     â”‚
â”‚   How can we improve?"                                 â”‚
â”‚                                                         â”‚
â”‚  GPT-4 Output: "Be more specific about clarity/depth" â”‚
â”‚                                                         â”‚
â”‚  Generate 2 Variations:                                â”‚
â”‚  V1: "Answer this question: {question}                â”‚
â”‚       with clarity and depth."                         â”‚
â”‚  V2: "Answer the question: {question}                 â”‚
â”‚       Be thorough and precise."                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Test Variant 1      â”‚  Test Variant 2                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ DNA: 0.85          â”‚ â€¢ DNA: 0.81                       â”‚
â”‚ â€¢ Photosynthesis: 0.89 (â†‘ BETTER!) â”‚ â€¢ Photosynthesis: 0.78 â”‚
â”‚ â€¢ ML: 0.88           â”‚ â€¢ ML: 0.79                        â”‚
â”‚                      â”‚                                    â”‚
â”‚ Avg: 0.87 âœ¨ GOOD   â”‚ Avg: 0.79  (No improvement)      â”‚
â”‚ Max: 0.89 âœ¨ BEST    â”‚ Max: 0.81                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  BEAM UPDATE (beam_width = 2)                           â”‚
â”‚  Keep Top 2 Prompts:                                    â”‚
â”‚                                                         â”‚
â”‚  POSITION 1 (Best):                                    â”‚
â”‚  "Answer this question: {question} with clarity..."    â”‚
â”‚  Max Reward: 0.89 âœ¨                                   â”‚
â”‚                                                         â”‚
â”‚  POSITION 2:                                           â”‚
â”‚  "Answer this question: {question}"                    â”‚
â”‚  Max Reward: 0.82                                      â”‚
â”‚                                                         â”‚
â”‚  REMOVED (didn't make top 2):                          â”‚
â”‚  "Answer the question: {question} Be thorough..."      â”‚
â”‚  Max Reward: 0.81                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [END OF beam_rounds=1]                                â”‚
â”‚  Training Complete!                                    â”‚
â”‚                                                         â”‚
â”‚  âœ¨ FINAL RESULT âœ¨                                    â”‚
â”‚  Best Prompt Found:                                    â”‚
â”‚  "Answer this question: {question} with clarity       â”‚
â”‚   and depth."                                          â”‚
â”‚                                                         â”‚
â”‚  Best Reward: 0.89 (vs baseline 0.78) â†‘ 14% IMPROVED  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
END: APO Training Complete
```

---

## ğŸ“Š Reward Landscape

```
                  Prompt Performance Distribution
                           
      0.95 â”‚                    
      0.90 â”‚              âœ¨ â† BEST FOUND: 0.89
      0.85 â”‚         â—†     
      0.80 â”‚    â—† âœ“ â—†      â† Baseline: 0.78
      0.75 â”‚ â—† â—†  â—† â—†   
      0.70 â”‚â—†  â—†  â—†       
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
             Template Variations
             
Legend:
  â—† = Template variation tested
  âœ“ = Baseline prompt (starting point)
  âœ¨ = Best prompt found by APO
```

---

## ğŸ” Selection Algorithm Visualization

### The `max()` function logic:

```python
# All templates and their maximum rewards
rewards_by_template = {
    "Template A": [0.75, 0.78, 0.71],      # max = 0.78
    "Template B": [0.82, 0.81, 0.79],      # max = 0.82
    "Template C": [0.89, 0.85, 0.87],      # max = 0.89 â† WINNER
    "Template D": [0.72, 0.74, 0.70],      # max = 0.74
    "Template E": [0.81, 0.80, 0.79],      # max = 0.81
}

                    â–¼ max(x[1]) Applied â–¼

Template A: max([0.75, 0.78, 0.71]) = 0.78 â”
Template B: max([0.82, 0.81, 0.79]) = 0.82 â”‚
Template C: max([0.89, 0.85, 0.87]) = 0.89 â”¼â”€â†’ BEST: 0.89
Template D: max([0.72, 0.74, 0.70]) = 0.74 â”‚
Template E: max([0.81, 0.80, 0.79]) = 0.81 â”˜

Result: Template C (the one with 0.89)
```

---

## ğŸ“ˆ Training Convergence Example

```
Round 0 (Baseline):
  Baseline Reward: 0.78
  Beam: [0.78]

Round 1:
  Baseline: 0.78
  Variant 1: 0.87 â†‘
  Variant 2: 0.79
  Beam: [0.87, 0.78]
  âœ“ Improvement found! (0.78 â†’ 0.87)

Round 2 (if beam_rounds > 1):
  Top Prompt: 0.87
  New Variant 1: 0.88 â†‘
  New Variant 2: 0.85
  Beam: [0.88, 0.87]
  âœ“ Small improvement (0.87 â†’ 0.88)

Round 3 (if beam_rounds > 2):
  Top Prompt: 0.88
  New Variant 1: 0.86 â†“
  New Variant 2: 0.87
  Beam: [0.88, 0.87]
  âœ— No improvement - optimization plateau reached

FINAL: Best = 0.88
```

---

## ğŸ”€ Beam Search Tree

```
                    Baseline: "Answer this: {q}"
                          (0.78)
                           â”‚
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
           â”‚               â”‚               â”‚
        Test1          Test2           Test3
        (0.82)         (0.75)          (0.80)
           â”‚
           â–¼
    Generate Variants
    (branch_factor=2)
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
 V1.1         V1.2
 (0.87)      (0.79)
  â”Œâ”          â””â”€ Dropped
  â”‚â”‚
  â”‚â””â”€ Select â† Beam position 1
  â”‚
  â””â”€ Top 2 kept in beam (beam_width=2)
     (0.87 moves to position 1 in next round)


Round 1 Beam: [(0.87, "with clarity..."), (0.78, baseline)]
â†“
Round 2: Generate from best
           â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”
    â”‚             â”‚
 V2.1         V2.2
 (0.88)      (0.85)
  âœ“           â””â”€ New beam position 2
  
Round 2 Beam: [(0.88, "with clarity and depth..."), (0.87, "with clarity...")]

Continue until beam_rounds exhausted or no improvement...
```

---

## ğŸ’¾ Memory Snapshot During Training

```
After 3 test cases with baseline:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ templates_tested = [                    â”‚
â”‚   "Answer this question: {question}"    â”‚
â”‚ ]                                       â”‚
â”‚                                         â”‚
â”‚ rewards_by_template = {                â”‚
â”‚   "Answer this question: {question}": [ â”‚
â”‚     0.82,  # DNA                        â”‚
â”‚     0.75,  # Photosynthesis             â”‚
â”‚     0.78   # Machine Learning           â”‚
â”‚   ]                                     â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

After testing variants:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ templates_tested = [                    â”‚
â”‚   "Answer this question: {question}",   â”‚
â”‚   "Answer this question: {question} withâ”‚
â”‚    clarity and depth.",                 â”‚
â”‚   "Answer the question: {question}      â”‚
â”‚    Be thorough and precise."            â”‚
â”‚ ]                                       â”‚
â”‚                                         â”‚
â”‚ rewards_by_template = {                â”‚
â”‚   "Answer this question: {question}": [ â”‚
â”‚     0.82, 0.75, 0.78                    â”‚
â”‚   ],                                    â”‚
â”‚   "Answer this question: {question} withâ”‚
â”‚    clarity and depth.": [               â”‚
â”‚     0.85, 0.89, 0.88                    â”‚ â† BEST
â”‚   ],                                    â”‚
â”‚   "Answer the question: {question}      â”‚
â”‚    Be thorough and precise.": [         â”‚
â”‚     0.81, 0.78, 0.79                    â”‚
â”‚   ]                                     â”‚
â”‚ }                                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ Final Selection Process

```
All Candidates:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Template 1: max reward = 0.82              â”‚
â”‚ Template 2: max reward = 0.89  â† SELECT   â”‚
â”‚ Template 3: max reward = 0.81              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Selection Code:
best = max(rewards_by_template.items(), key=lambda x: max(x[1]))
       â†“
       Returns: (Template 2, [0.85, 0.89, 0.88])
       â†“
best_score = max([0.85, 0.89, 0.88]) = 0.89

Output:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ† BEST PROMPT FOUND:                   â”‚
â”‚                                         â”‚
â”‚ "Answer this question: {question}       â”‚
â”‚  with clarity and depth."               â”‚
â”‚                                         â”‚
â”‚ âœ¨ Best Reward Score: 0.89             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“Š Comparison: Before vs After

```
Before APO:
  Baseline Prompt: "Answer this question: {question}"
  Average Performance: 0.78
  
  Q: "What is DNA?"
  A: "DNA is a molecule."
  Reward: 0.75 (short, missing keywords)

After APO:
  Optimized Prompt: "Answer this question: {question} 
                     with clarity and depth."
  Average Performance: 0.87 (+11% improvement!)
  
  Q: "What is DNA?"
  A: "DNA, or deoxyribonucleic acid, is the hereditary 
      material found in living organisms..."
  Reward: 0.89 (detailed, includes keywords)
```

---

## âš¡ Key Insights

1. **APO tests prompts methodically** - Not random, follows beam search
2. **Rewards guide selection** - Highest max reward wins
3. **Multiple tests per prompt** - Same prompt tested 3-8 times for stability
4. **Average vs Max** - Selection uses MAX (best case), summary shows both
5. **GPT-4 generates variants** - Not rule-based, uses LLM critique
6. **Convergence** - Training stops when beam rounds exhausted or no improvement

