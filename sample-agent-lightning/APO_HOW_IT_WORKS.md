# How APO (Automatic Prompt Optimization) Works

## Overview

APO is an algorithm that automatically optimizes LLM prompts by:
1. **Testing different prompts** on your validation dataset
2. **Calculating rewards** based on answer quality
3. **Using beam search** to explore promising prompt variations
4. **Selecting the best prompt** based on maximum reward

---

## ğŸ¯ The Reward System

### How Rewards Are Calculated

In `apo_training.py`, rewards are computed in the `calculate_reward()` function:

```python
def calculate_reward(answer: str, expected_keywords: list[str]) -> float:
    # Check keyword coverage (70% weight)
    keyword_score = sum(1 for keyword in expected_keywords if keyword.lower() in answer_lower) / max(len(expected_keywords), 1)
    
    # Bonus for detailed answers (30% weight)
    length_score = min(1.0, len(answer) / 200.0)
    
    # Weighted combination
    final_score = keyword_score * 0.7 + length_score * 0.3
    return min(1.0, final_score)
```

**Scoring Breakdown:**
- âœ… **0.8 - 1.0**: Green emoji - Excellent answer with key keywords
- âš¡ **0.5 - 0.8**: Yellow emoji - Good answer with most keywords
- âŒ **0.0 - 0.5**: Red emoji - Poor answer, missing keywords

---

## ğŸ” Beam Search Algorithm

APO uses **Beam Search** to efficiently explore the prompt space:

```
Round 1: Test Initial Prompt
         â†“
         Reward: 0.85
         â†“
Generate 2 Variations (branch_factor=2)
         â†“
    [Prompt v1]  [Prompt v2]
      0.78         0.91 âœ¨ (Best)
      â†“             â†“
  Keep Top 2 (beam_width=2)
      â†“
   [Prompt v2] â†’ Generate more variations...
```

### Key Hyperparameters

| Parameter | Value | Meaning |
|-----------|-------|---------|
| `val_batch_size` | 3 | Validate on 3 samples per round |
| `gradient_batch_size` | 2 | Use 2 samples to critique and improve |
| `beam_width` | 2 | Keep top 2 prompts in the beam |
| `branch_factor` | 2 | Generate 2 variations per prompt |
| `beam_rounds` | 1 | Run 1 round of optimization |

---

## ğŸ§  How APO Generates New Prompts

APO uses **GPT-4** to critique and improve prompts:

1. **Evaluate**: Run the current prompt on validation samples
2. **Score**: Calculate rewards for each sample
3. **Critique**: Send to GPT-4: "This prompt got a 0.75 reward. How can we improve it?"
4. **Generate**: GPT-4 creates 2 new prompt variations
5. **Compare**: Test both variants and keep the best ones

---

## ğŸ“Š Where Prompts Are Stored

### During Training

```python
templates_tested = []          # List of all unique templates tried
rewards_by_template = {}       # Dict mapping templates to their rewards
```

### In Memory

- **Trainer**: Holds current resources (including the current prompt)
- **Algorithm State**: APO maintains the beam of best prompts
- **Execution Traces**: Each rollout stores which prompt produced which reward

### No Persistent Storage

By default, prompts are **NOT saved to disk**. If you want to persist the best prompt:

```python
# Add this after training completes:
import json

best_template = max(rewards_by_template.items(), key=lambda x: max(x[1]))[0]
best_score = max(rewards_by_template[max(rewards_by_template.items(), key=lambda x: max(x[1]))[1]])

with open("best_prompt.json", "w") as f:
    json.dump({
        "template": best_template,
        "score": best_score
    }, f, indent=2)
```

---

## ğŸ“ˆ Training Output Explained

### Per-Template Output

```
======================================================================
ğŸ”¥ TEMPLATE BEING TESTED:
======================================================================
Answer this question: {question} with clarity and depth.
======================================================================

[Agent] ğŸ“ Q: What is DNA?...
[Agent] ğŸ’¬ A: DNA, or deoxyribonucleic acid, is a molecule...
[Agent] âœ… Reward: 0.825
```

**What this shows:**
- The exact template being tested (yellow)
- The question being asked
- The LLM's answer
- The reward score with color coding

### End-of-Training Summary

```
ğŸ“Š Summary: 5 unique templates were tested during training:
======================================================================

Template 1:
Answer this question: {question}
  ğŸ“ˆ Avg Reward: 0.782 | Max Reward: 0.825 | Tested: 8 times

Template 2:
Answer this question: {question} with clarity and depth.
  ğŸ“ˆ Avg Reward: 0.803 | Max Reward: 0.895 | Tested: 7 times

Template 3:
[Generated by GPT-4]
  ğŸ“ˆ Avg Reward: 0.759 | Max Reward: 0.812 | Tested: 6 times

...

======================================================================
ğŸ† BEST PROMPT FOUND:
======================================================================
Answer this question: {question} with clarity and depth.

âœ¨ Best Reward Score: 0.895
======================================================================
```

---

## ğŸ† Selection Process

APO selects the best prompt using:

```python
best_prompt = max(prompt_and_rewards, key=lambda x: x[1])
```

**Logic:**
1. For each template tested, track its maximum reward
2. Select the template with the highest max reward
3. That becomes your optimized prompt!

---

## ğŸ’¾ Practical Example: Persistence

To save and reuse the best prompt:

```python
import json
import agentlightning as agl

# After training completes
best_template = max(
    rewards_by_template.items(),
    key=lambda x: max(x[1])
)[0]

# Save it
with open("optimized_prompt.json", "w") as f:
    json.dump({"prompt": best_template}, f)

# Reload it for inference
with open("optimized_prompt.json", "r") as f:
    data = json.load(f)
    
optimized_prompt = agl.PromptTemplate(
    template=data["prompt"],
    engine="f-string"
)

# Use it!
response = client.chat.completions.create(
    messages=[{"role": "user", "content": optimized_prompt.format(question="What is ML?")}]
)
```

---

## ğŸ“ Key Concepts

| Concept | Explanation |
|---------|-------------|
| **Reward** | Score (0-1) measuring answer quality |
| **Template** | The prompt string with `{question}` placeholder |
| **Beam** | Collection of best prompts being tracked |
| **Round** | One cycle of testing, evaluating, and generating new prompts |
| **Variant** | A new prompt generated by GPT-4's critique |
| **Gradient** | The improvement signal: "this worked better because..." |

---

## âš¡ Quick Reference

**What APO Does:**
1. âœ… Tests multiple prompt variations
2. âœ… Scores each based on LLM answer quality
3. âœ… Uses beam search to explore efficiently
4. âœ… Generates new prompts via GPT-4
5. âœ… Returns the best prompt found

**What APO Doesn't Do:**
- âŒ Save prompts to disk automatically
- âŒ Train the LLM model itself
- âŒ Modify your validation data
- âŒ Cache responses between runs

