# One-Page Visual Summary: APO Prompt Selection & Storage

## ğŸ¯ THE CORE ANSWER

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 HOW APO SELECTS BEST PROMPT                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  1. Test multiple prompts on validation data                   â”‚
â”‚  2. Calculate reward score (0.0 - 1.0) for each               â”‚
â”‚  3. Find MAX reward for each unique prompt                     â”‚
â”‚  4. Select the prompt with HIGHEST max reward                 â”‚
â”‚                                                                 â”‚
â”‚  CODE:  best = max(rewards_by_template.items(),              â”‚
â”‚                   key=lambda x: max(x[1]))                    â”‚
â”‚                                                                 â”‚
â”‚  EXAMPLE:                                                       â”‚
â”‚  â€¢ Template A max: 0.82                                        â”‚
â”‚  â€¢ Template B max: 0.89  â† WINNER                             â”‚
â”‚  â€¢ Template C max: 0.75                                        â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ WHERE PROMPTS ARE STORED

```
DURING TRAINING (In Memory):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  templates_tested = [            â”‚
â”‚    "Answer this: {q}",           â”‚
â”‚    "Answer clearly: {q}",        â”‚
â”‚    "Answer thoroughly: {q}",     â”‚
â”‚  ]                               â”‚
â”‚                                  â”‚
â”‚  rewards_by_template = {         â”‚
â”‚    "Answer this: {q}": [0.78...] â”‚
â”‚    "Answer clearly: {q}": [0.89] â”‚ â† BEST
â”‚    "Answer thoroughly: {q}": [0.81] â”‚
â”‚  }                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

AFTER TRAINING (Nowhere):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  âŒ NOT saved to disk             â”‚
â”‚  âŒ NOT in database              â”‚
â”‚  âœ… Printed to console           â”‚
â”‚  âœ… Best shown in output         â”‚
â”‚                                  â”‚
â”‚  Must manually save with:        â”‚
â”‚  json.dump(..., "best.json")    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

INSIDE APO (Hidden):
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â€¢ Beam (top N prompts)         â”‚
â”‚  â€¢ Best found tracking          â”‚
â”‚  â€¢ Generation history           â”‚
â”‚  â†’ Not directly accessible      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ SIMPLIFIED PROCESS

```
START
  â†“
Test Baseline Prompt
  Reward: 0.78
  â†“
GPT-4: "This is generic, add 'clarity and depth'"
  â†“
Generate Variations (2 new prompts)
  â†“
Test V1: 0.87 âœ… BETTER   |  Test V2: 0.79 âŒ WORSE
  â†“
Keep V1 in beam (top 2 prompts)
  â†“
Continue? â†’ NO (beam_rounds=1)
  â†“
SELECT: Prompt with max reward (0.89)
  â†“
OUTPUT: "Answer: {q} with clarity and depth." â† Best
  â†“
END
```

---

## ğŸ“Š DATA STRUCTURE AT A GLANCE

```
After running apo_training.py:

templates_tested:        rewards_by_template:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ [               â”‚     â”‚ {                    â”‚
â”‚   "T1",         â”‚ â†’   â”‚   "T1": [0.78,0.75]  â”‚
â”‚   "T2",         â”‚     â”‚   "T2": [0.85,0.89]  â”‚ â† Highest max
â”‚   "T3"          â”‚     â”‚   "T3": [0.71,0.75]  â”‚
â”‚ ]               â”‚     â”‚ }                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Selection: max([0.78, 0.85, 0.71]) = 0.85 from T2? NO!
          max([0.78], [0.89], [0.75]) = 0.89 from T2? YES! âœ“
```

---

## ğŸ’» CODE TIMELINE

```
Line 30:    Global storage initialized
            templates_tested = []
            rewards_by_template = {}
                     â†“
Line 55:    During each rollout - template tracked
            if template not in templates_tested:
                templates_tested.append(template)
                     â†“
Line 105:   Rewards logged after evaluation
            rewards_by_template[template].append(reward)
                     â†“
Line 250:   Best prompt selected at end
            best = max(rewards_by_template.items(),
                      key=lambda x: max(x[1]))
                     â†“
Line 272:   Best prompt displayed to user
            print("ğŸ† BEST PROMPT FOUND:")
            print(best_template)
```

---

## âœ¨ CONSOLE OUTPUT LAYOUT

```
During Each Test:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ”¥ TEMPLATE BEING TESTED:             â”‚
â”‚ Answer: {question} with clarity       â”‚
â”‚ [Agent] ğŸ“ Q: What is DNA?...         â”‚
â”‚ [Agent] ğŸ’¬ A: DNA is a molecule...    â”‚
â”‚ [Agent] âœ… Reward: 0.825              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

End of Training:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸ“Š Summary: 3 templates tested        â”‚
â”‚                                        â”‚
â”‚ Template 1: "Answer: {q}"             â”‚
â”‚   Avg: 0.783 | Max: 0.82              â”‚
â”‚                                        â”‚
â”‚ Template 2: "Answer: {q} with..."     â”‚
â”‚   Avg: 0.873 | Max: 0.89              â”‚
â”‚                                        â”‚
â”‚ Template 3: "Answer {q} thoroughly"   â”‚
â”‚   Avg: 0.793 | Max: 0.81              â”‚
â”‚                                        â”‚
â”‚ ğŸ† BEST PROMPT FOUND:                 â”‚
â”‚ Answer: {question} with clarity...    â”‚
â”‚ Score: 0.89 âœ¨                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ¯ KEY FORMULA

```
                  SELECT BEST PROMPT
                          â†“
        best = max(rewards_by_template.items(),
                   key=lambda x: max(x[1]))
                    â†“        â†“         â†“
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”˜        â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”
            â–¼                â–¼                   â–¼
      For each template,  Find its max    Compare max values
      rewards is a list    reward value    & return highest
      
            Example:
            Template A: [0.78, 0.75, 0.82] max=0.82
            Template B: [0.85, 0.89, 0.88] max=0.89 â† SELECTED
            Template C: [0.71, 0.75, 0.70] max=0.75
```

---

## ğŸ“ HOW TO USE BEST PROMPT

### Option 1: Copy from Console
```
Look for: ğŸ† BEST PROMPT FOUND:
          Answer this question: {question} with clarity and depth.
Copy this and use it!
```

### Option 2: Get from Code
```python
# After training completes:
best_template = max(
    rewards_by_template.items(),
    key=lambda x: max(x[1])
)[0]
print(best_template)
```

### Option 3: Save to File
```python
import json
best_template = max(rewards_by_template.items(),
                   key=lambda x: max(x[1]))[0]
with open("best.json", "w") as f:
    json.dump({"prompt": best_template}, f)
```

---

## âš ï¸ IMPORTANT NOTES

âœ… **DO THIS:**
- Save best prompt to file when training completes
- Look at console output for highlighted templates
- Use max() function for selection

âŒ **DON'T DO THIS:**
- Assume prompts are saved automatically (they're not)
- Try to access APO's internal beam state directly
- Use average reward for selection (use max!)

---

## ğŸ“š FOR MORE DETAILS

| If you want... | Read this file |
|---|---|
| 5-min explanation | ANSWER_APO_STORAGE_SELECTION.md |
| Quick reference | APO_STORAGE_SUMMARY.md |
| Full algorithm | APO_HOW_IT_WORKS.md |
| Code locations | APO_CODE_REFERENCE.md |
| Visual diagrams | APO_VISUALIZATION.md |
| Technical deep dive | APO_STORAGE_DETAILS.md |
| Navigation guide | APO_DOCS_INDEX.md |

---

**TL;DR:** APO selects the prompt with highest max reward. Prompts are stored in memory during training. Save them manually before script ends!

